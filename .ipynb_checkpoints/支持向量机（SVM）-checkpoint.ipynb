{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量机（SVM）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM是一种非常强大的分类算法，不仅仅会进行分类，而且还会寻找两分类保持最大间隔的界线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "如下图所示，左右两条线都可以把数据分为两类，那么这两条线，那种更好呢？\n",
    "![UrTkVK.png](https://s1.ax1x.com/2020/07/16/UrTkVK.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "当然，这里答案是左边这条，因为在左图中，两个类别的点到分界线的间距适当，而右图中则有的距离远，有的距离近。而且，比较两图中点与线的最小间距，也可以发现左图的最小间距要比右图大。（也就是说，左图可以尽可能的把两分类分离得更远）支持向量机就是按照左图的样式去工作的。\n",
    "\n",
    "![UrTm2d.png](https://s1.ax1x.com/2020/07/16/UrTm2d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 误差函数（Error Function）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们再来回忆下，什么是分类问题，其实就是找到一条线（分类函数，如下图实线）把这些蓝点和红点尽可能地分开，那我们为了获得更好的分类效果（泛化能力），那就想要让这些点到线的距离越大越好，所以，我们可以做两条平行线（下图虚线），让这两条边界线的距离（Margin）达到最大，那么就可以达到最佳的分类效果。\n",
    "![U2djTs.png](https://s1.ax1x.com/2020/07/18/U2djTs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在该分类中，我们不仅希望能把所有颜色的点都分到正确的类别中，还希望在中间的边界区域（Margin）中没有任何点。\n",
    "\n",
    "这样就可以很清晰的得到，SVM分类算法的总误差为**分类误差**（把红点分到蓝色部分，蓝点分到红色部分，即下图中虚线外侧的分类错误点）和 **边界误差**（出现在Margin部分的点）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而我们在拟合数据（训练模型）的过程，就是在不断降低总误差的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UWnGHU.png](https://s1.ax1x.com/2020/07/19/UWnGHU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类误差（Classification Error）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下图所示，我们通过一个示例来看下如何计算分类误差。对于某一类别的分类误差，可以按照如下方式进行计算：\n",
    "- 以远离该分类的边界线作为基准线\n",
    "- 计算分类错误的点到该基准线的距离\n",
    "\n",
    "最后，将两个类别的误差相加，即可得到分类误差。如下图所示，红色类别的分类误差为：0.5+1.5+3.5 = 5.5;蓝色类别的分类误差为0.3+2+3=5.3，所以总误差为10.8.\n",
    "\n",
    "![UWuxWd.png](https://s1.ax1x.com/2020/07/19/UWuxWd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 边界误差（Margin Error）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在对模型进行拟合时，是希望边界距离，也就是Margin的宽度越大越好。也就是说，我们需要定义一个边界误差，让边界距离越大的时候，边界误差越小，而边界距离越小的时候，边界误差就越大。\n",
    "\n",
    "如下图所示，因为两条边界线平行，所以它们之间的距离$\\frac{|b-1-(b+1)|}{\\sqrt{W^{2}+0^2}} = \\frac{2}{|W|}$。所以刚好拟合方程中$|W|$的大小跟距离是成反比的，这刚好符合我们对边界误差的需求，所以，我们可以把边界误差定义为$|W|^{2}$。\n",
    "\n",
    "![UW15Ks.png](https://s1.ax1x.com/2020/07/19/UW15Ks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算完分类误差与边界误差之和，我们便可以使用梯度下降法去优化该模型了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型的灵活性（C parameter）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "对于下图中的两个模型，思考下，哪个更好一些呢？\n",
    "![UW3HOA.png](https://s1.ax1x.com/2020/07/19/UW3HOA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "这里其实并没有正确答案，因为要考虑实际的业务情况。比如，对于医学检测上，我们不想要有任何分类错误，那么右边的模型可能更好些；对于其他的一些情况，我们可以接受一些分类错误，但想要尽可能大的分界空间，那么左边的模型可能更好些。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，我们该如何在训练模型时，更加灵活的控制分类误差与边界误差的优化优先级呢？答案就是使用一个参数去控制二者在总误差中的占比，这个参数就是C Parameter。\n",
    "\n",
    "$$Error = C \\cdot Classification Error + Margin Error$$\n",
    "\n",
    "那么，当C比较大时，分类误差的占比更大，就会在训练模型时倾向于获取更低的分类误差；反之亦凡。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多项式核技巧（Polynomial Kernel）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于如下的情况，我们可以很轻松的使用一条直线将两个分类分开：\n",
    "![UWdTl6.png](https://s1.ax1x.com/2020/07/19/UWdTl6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那对于如下情况，我们该怎么做呢？\n",
    "\n",
    "![UWwp1P.png](https://s1.ax1x.com/2020/07/19/UWwp1P.png)\n",
    "\n",
    "我们可以将这一维（只有x轴）的数据，扩展到二维空间（添加一个y轴），然后将这些点映射到一个二次函数$y=x^2$上，这样我们就可以很轻松地使用y=4这条直线将两个分类分开。\n",
    "![UWdL0e.png](https://s1.ax1x.com/2020/07/19/UWdL0e.png)\n",
    "然后，我们再把二次函数与分界线的交点，投射到一维的x轴上，得到$x=2和x=-2$两个点，这样就可以得到最佳的分裂点了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zhejiushi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
