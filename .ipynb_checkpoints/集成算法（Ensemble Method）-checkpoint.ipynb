{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前最流行的两种集成算法分别为boosting和bagging。\n",
    "- bagging ： 是bootstrap aggregatin（自助聚集）的简称。比如说，我们参加一次考试，但不知道该怎么做，这时候求助了几位朋友，对于数字型的问题，我们综合所有朋友的答案，求得均值；对于选择题，我们则将所有朋友的答案进行投票，选择票数最高的那个答案；\n",
    "\n",
    "- boosting ： 即提升算法；这里同样也是求助朋友，只不过我们依据不同的问题，求助不同专业的朋友，比如对于音乐类的题，我们就求助音乐专业的同学，对于计算机问题则求助计算机专业的同学。\n",
    "\n",
    "将如上的例子映射到机器学习中，那么这些“朋友“被称作”弱学习器“（并不表示模型是最弱的，只是相对于想要合成的强分类器而言较弱）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么需要进行模型集成？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练最佳机器学习模型的过程中，有两个相悖的影响因素，即偏差（bias）与方差（variance），这经常会在面试中会被问到。\n",
    "\n",
    "- Bias：当一个模型具有较高的bias时，这说明模型并不能很好的拟合数据（bending to data，也就是随着数据的变化而变化），如线性回归模型就是一个经常出现高bias的模型，如下图所示，它对于不同的数据都倾向于预测为同一条拟合线：\n",
    "![amt64H.png](https://s1.ax1x.com/2020/07/29/amt64H.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance：当一个模型具有较高的Variance时，表示该模型为了满足数据中每一点的需求，导致预测值变化非常大。线性回归模型经常是一种低Variance高Bias的模型。一种具有高Variance低Bias的模型就是决策树（尤其是没有早停early stop的决策树），模型会尽可能地根据数据切分再切分。\n",
    "![amaJUS.png](https://s1.ax1x.com/2020/07/29/amaJUS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，采用多个子模型集成的方式，我们可以更好得在Bias与Variance中找到一个平衡的中间位置。当然，还有一些改进模型的其他方式，这些都是基于数学的最小化bias或者Variance的算法，比如中心极限定理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进行模型集成以前，给高Variance的模型引入一定的随机变化，便可以降低Variance。主要可以通过如下两种方式：\n",
    "- Boostrap the data：就是有放回的进行数据抽样，然后使用抽样后的数据进行模型拟合；\n",
    "- subset the features：在决策树的每次分裂时或者在每个弱学习器中，使用特征集合的子集去拟合。\n",
    "\n",
    "而使用如上方式的算法，便是随机森林（random forest）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例\n",
    "决策树更倾向于去“记忆”数据，很容易产生过拟合；使用如上算法，选择多个特征集合的子集去训练多个决策树，最终将所有决策树的结果进行投票，得到预测结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bagging算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们有一组如下的数据，数据量很大，所以，我们并不想按照上面的算法去使用全部数据训练多个模型，这样会耗费很大算力。\n",
    "![amDxN6.png](https://s1.ax1x.com/2020/07/29/amDxN6.png)\n",
    "\n",
    "这时候，我们可以有放回的对数据进行抽样（也就是说有可能有样本会取到多次，也有可能一次都没取到），然后用每个抽样后的数据去训练模型，最终将所有模型集成到一起，进行投票，得到最终结果。\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=https://s1.ax1x.com/2020/07/29/amrKgg.png border=0></td>\n",
    "<td><img src=https://s1.ax1x.com/2020/07/29/amroVI.png border=0></td>\n",
    "<td><img src=https://s1.ax1x.com/2020/07/29/amsZL9.png border=0></td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align:center\">弱学习器A</td>\n",
    "<td style=\"text-align:center\">弱学习器B</td>\n",
    "<td style=\"text-align:center\">弱学习器C</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "三者合并，得到：\n",
    "\n",
    "![amsgwn.png](https://s1.ax1x.com/2020/07/29/amsgwn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
